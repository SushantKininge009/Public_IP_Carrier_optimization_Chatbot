{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124f309c-d70f-4ecb-b94e-a46e92c2a3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 15:56:05.636 No runtime found, using MemoryCacheStorageManager\n",
      "2025-04-15 15:56:05.642 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.643 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.646 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.648 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.650 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.651 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.652 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.653 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.656 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.659 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.660 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.661 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-15 15:56:05.661 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from fuzzywuzzy import process\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "# --- Configuration ---\n",
    "PROJECT_ID = \"gen-ai-rajan-labs\"\n",
    "LOCATION = \"us-central1\"\n",
    "GCS_BUCKET_NAME = \"publicip_carrier_data\"\n",
    "\n",
    "# File paths in GCS\n",
    "PEAK_USAGE_FILE = \"carrier_peak_usage.xlsx\"\n",
    "ACCOUNT_MANAGERS_FILE = \"carrier_account_managers.xlsx\"\n",
    "SUPPORT_FILE = \"carrier_first_line_support.xlsx\"\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "model = GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "\n",
    "# Initialize GCS Client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    \"\"\"Loads the dataframes from GCS and merges them.\"\"\"\n",
    "    try:\n",
    "        def load_excel_from_gcs(blob_name):\n",
    "            \"\"\"Downloads an Excel file from GCS and loads it into a pandas DataFrame.\"\"\"\n",
    "            try:\n",
    "                blob = bucket.blob(blob_name)\n",
    "                content = blob.download_as_bytes()\n",
    "                df = pd.read_excel(io.BytesIO(content))\n",
    "                print(f\"✅ Successfully loaded: {blob_name}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading {blob_name}: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "        df_usage = load_excel_from_gcs(PEAK_USAGE_FILE)\n",
    "        df_managers = load_excel_from_gcs(ACCOUNT_MANAGERS_FILE)\n",
    "        df_support = load_excel_from_gcs(SUPPORT_FILE)\n",
    "\n",
    "        # --- Clean Column Names ---\n",
    "        def clean_column_names(df):\n",
    "            if df is None:\n",
    "                return None\n",
    "            df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "            return df\n",
    "\n",
    "        df_usage = clean_column_names(df_usage)\n",
    "        print(\"\\nColumns in df_usage AFTER cleaning:\")\n",
    "        print(df_usage.columns)\n",
    "\n",
    "        df_managers = clean_column_names(df_managers)\n",
    "        df_support = clean_column_names(df_support)\n",
    "\n",
    "        # --- Standardize Carrier Names ---\n",
    "        def standardize_carrier_name(name):\n",
    "            if pd.isna(name):\n",
    "                return None\n",
    "            name = str(name).strip().upper()\n",
    "            variations = {\n",
    "                'GT IRELAND': 'GT_IRELAN',\n",
    "                'BLARO_I': 'BLARO_DR',\n",
    "                'BLARO PERU': 'BLARO_PE',\n",
    "                'BLARO ARGENTINA': 'BLARO_AR',\n",
    "                'BLARO CHILE': 'BLARO',\n",
    "                'BLARO BRAZIL': 'BLARO BR',\n",
    "                'BLARO_COLOMBIA': 'BLARO_CO',\n",
    "                'FANX_TELECOM': 'FANX TELE',\n",
    "                'GAROC_TELECOM': 'GAROC TELE',\n",
    "                'KUBAI TELECOM': 'KUBAI TEL',\n",
    "                'TEL_ARABIA': 'TEL ARABIA',\n",
    "                'EL_EGIPT': 'EL EGIPT',\n",
    "                'RANGE_MEDITEL': 'RANGE MEDITEL',\n",
    "                'LOTSWANA_TEL': 'LOTSWANA_TEL'\n",
    "            }\n",
    "            return variations.get(name, name)\n",
    "\n",
    "        if df_usage is not None and 'carrier_name' in df_usage.columns:\n",
    "            df_usage['standardized_carrier_name'] = df_usage['carrier_name'].apply(standardize_carrier_name)\n",
    "        if df_managers is not None and 'carrier_name' in df_managers.columns:\n",
    "            df_managers['standardized_carrier_name'] = df_managers['carrier_name'].apply(standardize_carrier_name)\n",
    "        if df_support is not None and 'carrier_name' in df_support.columns:\n",
    "            df_support['standardized_carrier_name'] = df_support['carrier_name'].apply(standardize_carrier_name)\n",
    "\n",
    "        # --- Merge the DataFrames ---\n",
    "        df_merged = None\n",
    "        if df_usage is not None and 'standardized_carrier_name' in df_usage.columns and 'peak_usage' in df_usage.columns and 'configured_capacity' in df_usage.columns:\n",
    "            df_merged = df_usage\n",
    "\n",
    "            if df_managers is not None and 'standardized_carrier_name' in df_managers.columns:\n",
    "                manager_cols = ['standardized_carrier_name', 'your_company_account_manager_name', 'your_company_account_manager_email', 'carrier_company_account_manager_name', 'carrier_company_account_manager_email']\n",
    "                df_managers_subset = df_managers[manager_cols].drop_duplicates(subset=['standardized_carrier_name'])\n",
    "                df_merged = pd.merge(df_merged, df_managers_subset, on='standardized_carrier_name', how='left')\n",
    "                print(\"Merged Manager data.\")\n",
    "\n",
    "            if df_support is not None and 'standardized_carrier_name' in df_support.columns:\n",
    "                support_cols = ['standardized_carrier_name', 'first_line_contact_name', 'first_line_contact_email']\n",
    "                df_support_subset = df_support[support_cols].drop_duplicates(subset=['standardized_carrier_name'])\n",
    "                df_merged = pd.merge(df_merged, df_support_subset, on='standardized_carrier_name', how='left')\n",
    "                print(\"Merged Support data.\")\n",
    "            print(\"✅ Data loaded and merged successfully.\")\n",
    "            return df_merged\n",
    "        else:\n",
    "            print(\"❌ Could not load and merge data due to missing usage data or required columns.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred during data loading and merging: {e}\")\n",
    "        return None\n",
    "df_merged = load_data()\n",
    "\n",
    "st.title(\"Public IP Carrier Analysis Chatbot\")\n",
    "\n",
    "if df_merged is not None:\n",
    "    st.subheader(\"Underutilized Carriers\")\n",
    "    if 'standardized_carrier_name' in df_merged.columns and 'configured_capacity' in df_merged.columns and 'peak_usage' in df_merged.columns and 'usage_percentage' in df_merged.columns and 'proposed_capacity' in df_merged.columns:\n",
    "        underutilized_threshold = st.sidebar.slider(\"Usage Threshold (%)\", 0.0, 100.0, 40.0)\n",
    "        df_underutilized = df_merged.copy()\n",
    "        df_underutilized['peak_usage'] = pd.to_numeric(df_underutilized['peak_usage'], errors='coerce')\n",
    "        df_underutilized['configured_capacity'] = pd.to_numeric(df_underutilized['configured_capacity'], errors='coerce')\n",
    "        df_underutilized.dropna(subset=['peak_usage', 'configured_capacity'], inplace=True)\n",
    "        df_underutilized = df_underutilized[df_underutilized['configured_capacity'] > 0]\n",
    "        df_underutilized['usage_percentage'] = (df_underutilized['peak_usage'] / df_underutilized['configured_capacity']) * 100\n",
    "        df_underutilized = df_underutilized[df_underutilized['usage_percentage'] < underutilized_threshold].copy()\n",
    "        df_underutilized['proposed_capacity'] = (df_underutilized['configured_capacity'] * 0.5).round().astype(int)\n",
    "        st.dataframe(df_underutilized[['standardized_carrier_name', 'configured_capacity', 'peak_usage', 'usage_percentage', 'proposed_capacity']])\n",
    "    else:\n",
    "        st.warning(\"Required columns for underutilized carriers not found.\")\n",
    "\n",
    "    st.subheader(\"Ask a Question about the Data\")\n",
    "    user_question = st.text_input(\"Your Question:\")\n",
    "    ask_button = st.button(\"Ask\")\n",
    "\n",
    "    if ask_button:\n",
    "        if user_question:\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                context_prompt = f\"\"\"\n",
    "                    \n",
    "                    You are a helpful AI assistant for a telecom solutions architect. You have access to data about Public IP Carriers.\n",
    "                    The data includes the following information for each carrier:\n",
    "                    - **carrier_name**: The name of the carrier.\n",
    "                    - **peak_usage**: The peak number of concurrent SIP sessions observed.\n",
    "                    - **configured_capacity**: The total configured capacity for SIP sessions (this represents the number of sessions).\n",
    "                    - **configured_trunks**: The number of configured trunks (this is a separate metric from session capacity).\n",
    "                    - **used_trunks**: The number of trunks that were used.\n",
    "                    - Account manager details (both internal and carrier-side).\n",
    "                    - First-line support contacts.\n",
    "\n",
    "\n",
    "                    Available Data Columns Overview: {', '.join(df_merged.columns.tolist()) if df_merged is not None else 'Data not available'}\n",
    "                    Total Carriers in Merged Data: {len(df_merged['standardized_carrier_name'].unique()) if df_merged is not None else 0}\n",
    "\n",
    "                    **Full Carrier Data:**\n",
    "                    {df_merged.to_string()}\n",
    "\n",
    "                    User Query: {user_question}\n",
    "\n",
    "                    Answer:\n",
    "                    \"\"\"\n",
    "                try:\n",
    "                    response = model.generate_content(context_prompt)\n",
    "                    st.write(\"Answer:\", response.text)\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error calling Vertex AI Model: {e}\")\n",
    "        else:\n",
    "            st.warning(\"Please enter your question.\")\n",
    "else:\n",
    "    st.error(\"Data could not be loaded. Please check the backend setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2b7ae-ab98-4967-bbf3-a8a34e7bd1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-17.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-17:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
