{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124f309c-d70f-4ecb-b94e-a46e92c2a3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 12:04:53.564 No runtime found, using MemoryCacheStorageManager\n",
      "2025-04-11 12:04:53.569 No runtime found, using MemoryCacheStorageManager\n",
      "2025-04-11 12:04:53.570 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.572 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.573 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. Vertex AI SDK and GCS Client Initialized.\n",
      "✅ Successfully loaded: carrier_peak_usage.xlsx\n",
      "✅ Successfully loaded: carrier_account_managers.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 12:04:53.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.828 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.829 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.830 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.832 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.833 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.835 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.836 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.837 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.838 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.840 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.841 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.842 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.843 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.845 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.846 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.847 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.848 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.849 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-11 12:04:53.849 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded: carrier_first_line_support.xlsx\n",
      "Merged Manager data.\n",
      "Merged Support data.\n",
      "✅ Data loaded and merged successfully.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import io\n",
    "import re # For potential regex cleaning\n",
    "from fuzzywuzzy import process # For fuzzy name matching\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "# --- Configuration ---\n",
    "PROJECT_ID = \"gen-ai-rajan-labs\"  # Replace with your Project ID\n",
    "LOCATION = \"us-central1\"  # e.g., \"us-central1\"\n",
    "GCS_BUCKET_NAME = \"publicip_carrier_data\" # Replace with your bucket name\n",
    "\n",
    "# File paths in GCS\n",
    "PEAK_USAGE_FILE = \"carrier_peak_usage.xlsx\"\n",
    "ACCOUNT_MANAGERS_FILE = \"carrier_account_managers.xlsx\"\n",
    "SUPPORT_FILE = \"carrier_first_line_support.xlsx\"\n",
    "\n",
    "# Analysis Parameters\n",
    "USAGE_THRESHOLD_PERCENT = 40.0\n",
    "CAPACITY_REDUCTION_FACTOR = 0.5\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Load the Gemini model\n",
    "model = GenerativeModel(\"gemini-1.5-pro-002\") # Or choose a newer/different version if needed\n",
    "\n",
    "# Initialize GCS Client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
    "print(\"Setup Complete. Vertex AI SDK and GCS Client Initialized.\")\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    \"\"\"Loads the dataframes from GCS, cleans, standardizes, and merges them.\"\"\"\n",
    "    try:\n",
    "        def load_excel_from_gcs(blob_name):\n",
    "            \"\"\"Downloads an Excel file from GCS and loads it into a pandas DataFrame.\"\"\"\n",
    "            try:\n",
    "                blob = bucket.blob(blob_name)\n",
    "                content = blob.download_as_bytes()\n",
    "                df = pd.read_excel(io.BytesIO(content))\n",
    "                print(f\"✅ Successfully loaded: {blob_name}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading {blob_name}: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "        df_usage = load_excel_from_gcs(PEAK_USAGE_FILE)\n",
    "        df_managers = load_excel_from_gcs(ACCOUNT_MANAGERS_FILE)\n",
    "        df_support = load_excel_from_gcs(SUPPORT_FILE)\n",
    "\n",
    "        def clean_column_names(df):\n",
    "            \"\"\"Standardizes column names (lowercase, replace spaces with underscores).\"\"\"\n",
    "            if df is None:\n",
    "                return None\n",
    "            df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "            return df\n",
    "\n",
    "        df_usage = clean_column_names(df_usage)\n",
    "        df_managers = clean_column_names(df_managers)\n",
    "        df_support = clean_column_names(df_support)\n",
    "\n",
    "        def standardize_carrier_name(name):\n",
    "            if pd.isna(name):\n",
    "                return None\n",
    "            name = str(name).strip().upper()\n",
    "            variations = {\n",
    "                'GT IRELAND': 'GT_IRELAN',\n",
    "                'BLARO_I': 'BLARO_DR',\n",
    "                'BLARO PERU': 'BLARO_PE',\n",
    "                'BLARO ARGENTINA': 'BLARO_AR',\n",
    "                'BLARO CHILE': 'BLARO',\n",
    "                'BLARO BRAZIL': 'BLARO BR',\n",
    "                'BLARO_COLOMBIA': 'BLARO_CO',\n",
    "                'FANX_TELECOM': 'FANX TELE',\n",
    "                'GAROC_TELECOM': 'GAROC TELE',\n",
    "                'KUBAI TELECOM': 'KUBAI TEL',\n",
    "                'TEL_ARABIA': 'TEL ARABIA',\n",
    "                'EL_EGIPT': 'EL EGIPT',\n",
    "                'RANGE_MEDITEL': 'RANGE MEDITEL',\n",
    "                'LOTSWANA_TEL': 'LOTSWANA_TEL'\n",
    "            }\n",
    "            return variations.get(name, name)\n",
    "\n",
    "        if df_usage is not None and 'carrier_name' in df_usage.columns:\n",
    "            df_usage['standardized_carrier_name'] = df_usage['carrier_name'].apply(standardize_carrier_name)\n",
    "        else:\n",
    "            df_usage['standardized_carrier_name'] = None\n",
    "\n",
    "        if df_managers is not None and 'carrier_name' in df_managers.columns:\n",
    "            df_managers['standardized_carrier_name'] = df_managers['carrier_name'].apply(standardize_carrier_name)\n",
    "        else:\n",
    "            df_managers['standardized_carrier_name'] = None\n",
    "\n",
    "        if df_support is not None and 'carrier_name' in df_support.columns:\n",
    "            df_support['standardized_carrier_name'] = df_support['carrier_name'].apply(standardize_carrier_name)\n",
    "        else:\n",
    "            df_support['standardized_carrier_name'] = None\n",
    "\n",
    "        df_merged = None\n",
    "        if df_usage is not None and 'standardized_carrier_name' in df_usage.columns:\n",
    "            df_merged = df_usage\n",
    "\n",
    "            if df_managers is not None and 'standardized_carrier_name' in df_managers.columns:\n",
    "                manager_cols = ['standardized_carrier_name', 'your_company_account_manager_name', 'your_company_account_manager_email', 'carrier_company_account_manager_name', 'carrier_company_account_manager_email']\n",
    "                df_managers_subset = df_managers[manager_cols].drop_duplicates(subset=['standardized_carrier_name'])\n",
    "                df_merged = pd.merge(df_merged, df_managers_subset, on='standardized_carrier_name', how='left')\n",
    "                print(\"Merged Manager data.\")\n",
    "\n",
    "            if df_support is not None and 'standardized_carrier_name' in df_support.columns:\n",
    "                support_cols = ['standardized_carrier_name', 'first_line_contact_name', 'first_line_contact_email']\n",
    "                df_support_subset = df_support[support_cols].drop_duplicates(subset=['standardized_carrier_name'])\n",
    "                df_merged = pd.merge(df_merged, df_support_subset, on='standardized_carrier_name', how='left')\n",
    "                print(\"Merged Support data.\")\n",
    "            print(\"✅ Data loaded and merged successfully.\")\n",
    "            return df_merged\n",
    "        else:\n",
    "            print(\"❌ Could not load and merge data due to missing usage data or required columns.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred during data loading and merging: {e}\")\n",
    "        return None\n",
    "\n",
    "df_merged = load_data()\n",
    "\n",
    "st.title(\"Public IP Carrier Analysis Chatbot\")\n",
    "\n",
    "if df_merged is not None:\n",
    "    st.subheader(\"Underutilized Carriers\")\n",
    "    if 'standardized_carrier_name' in df_merged.columns and 'configured_capacity' in df_merged.columns and 'peak_usage' in df_merged.columns and 'usage_percentage' in df_merged.columns and 'proposed_capacity' in df_merged.columns:\n",
    "        underutilized_threshold = st.sidebar.slider(\"Usage Threshold (%)\", 0.0, 100.0, 40.0)\n",
    "        df_underutilized = df_merged.copy()  # Perform analysis on df_merged\n",
    "        df_underutilized['peak_usage'] = pd.to_numeric(df_underutilized['peak_usage'], errors='coerce')\n",
    "        df_underutilized['configured_capacity'] = pd.to_numeric(df_underutilized['configured_capacity'], errors='coerce')\n",
    "        df_underutilized.dropna(subset=['peak_usage', 'configured_capacity'], inplace=True)\n",
    "        df_underutilized = df_underutilized[df_underutilized['configured_capacity'] > 0]\n",
    "        df_underutilized['usage_percentage'] = (df_underutilized['peak_usage'] / df_underutilized['configured_capacity']) * 100\n",
    "        df_underutilized = df_underutilized[df_underutilized['usage_percentage'] < underutilized_threshold].copy()\n",
    "        df_underutilized['proposed_capacity'] = (df_underutilized['configured_capacity'] * CAPACITY_REDUCTION_FACTOR).round().astype(int)\n",
    "        st.dataframe(df_underutilized[['standardized_carrier_name', 'configured_capacity', 'peak_usage', 'usage_percentage', 'proposed_capacity']].head())\n",
    "    else:\n",
    "        st.warning(\"Required columns for underutilized carriers not found.\")\n",
    "\n",
    "    st.subheader(\"Ask a Question about the Data\")\n",
    "    user_question = st.text_input(\"Your Question:\")\n",
    "    ask_button = st.button(\"Ask\")\n",
    "\n",
    "    if ask_button:\n",
    "        if user_question:\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                context_prompt = f\"\"\"\n",
    "                You are a helpful AI assistant for a telecom solutions architect. You have access to data about Public IP Carriers.\n",
    "                The data includes carrier names, peak SIP session usage, configured capacity, account manager details (both internal and carrier-side), and first-line support contacts.\n",
    "                Carrier names have been standardized.\n",
    "\n",
    "                Here is a sample of the data:\n",
    "                {df_merged.head().to_string()}\n",
    "\n",
    "                Answer the user's query: {user_question}\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    response = model.generate_content(context_prompt)\n",
    "                    st.write(\"Answer:\", response.text)\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error calling Vertex AI Model: {e}\")\n",
    "        else:\n",
    "            st.warning(\"Please enter your question.\")\n",
    "else:\n",
    "    st.error(\"Data could not be loaded. Please check the backend setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2b7ae-ab98-4967-bbf3-a8a34e7bd1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-17.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-17:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
