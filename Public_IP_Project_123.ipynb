{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c33fd16a-74bd-4db6-a46e-e5b7f4e5c62f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.88.0)\n",
      "Requirement already satisfied: gcsfs in /opt/conda/lib/python3.10/site-packages (2025.2.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (2025.2.0)\n",
      "Requirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: streamlit in /opt/conda/lib/python3.10/site-packages (1.44.1)\n",
      "Requirement already satisfied: python-Levenshtein in /opt/conda/lib/python3.10/site-packages (0.27.1)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-google-vertexai in /opt/conda/lib/python3.10/site-packages (2.0.19)\n",
      "Requirement already satisfied: db-dtypes in /opt/conda/lib/python3.10/site-packages (1.4.1)\n",
      "Requirement already satisfied: fuzzywuzzy[speedup] in /opt/conda/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.38.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.24.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.26.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.7)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (3.9.5)\n",
      "Requirement already satisfied: decorator>4.1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.10/site-packages (from gcsfs) (1.2.1)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.2.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /opt/conda/lib/python3.10/site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/conda/lib/python3.10/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein) (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/conda/lib/python3.10/site-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.13.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.3.30)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from langchain-google-vertexai) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain-google-vertexai) (0.4.0)\n",
      "Requirement already satisfied: validators<0.35.0,>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from langchain-google-vertexai) (0.34.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.18.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (1.25.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.66.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.65.5)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.49.0rc1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.12.7)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (4.8.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.20)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.2.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas google-cloud-storage google-cloud-aiplatform gcsfs fsspec openpyxl streamlit fuzzywuzzy[speedup] python-Levenshtein langchain langchain-google-vertexai db-dtypes # db-dtypes sometimes needed by pandas agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b68fc70-0519-4a49-b475-a6f0d093e50a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3454bff-1a50-4d80-9a4b-368e44315063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. Vertex AI SDK and GCS Client Initialized.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (if not already on the Vertex AI Workbench instance)\n",
    "# %pip install pandas openpyxl google-cloud-storage google-cloud-aiplatform fuzzywuzzy[speedup]\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import re # For potential regex cleaning\n",
    "from fuzzywuzzy import process # For fuzzy name matching\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "# --- Configuration ---\n",
    "PROJECT_ID = \"gen-ai-rajan-labs\"  # Replace with your Project ID\n",
    "LOCATION = \"us-central1\"  # e.g., \"us-central1\"\n",
    "GCS_BUCKET_NAME = \"publicip_carrier_data\" # Replace with your bucket name\n",
    "GCS_BUCKET_NAME = \"publicip_carrier_data\" \n",
    "\n",
    "# File paths in GCS\n",
    "PEAK_USAGE_FILE = \"carrier_peak_usage.xlsx\"\n",
    "ACCOUNT_MANAGERS_FILE = \"carrier_account_managers.xlsx\"\n",
    "SUPPORT_FILE = \"carrier_first_line_support.xlsx\"\n",
    "\n",
    "# Analysis Parameters\n",
    "USAGE_THRESHOLD_PERCENT = 40.0\n",
    "CAPACITY_REDUCTION_FACTOR = 0.5\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Load the Gemini model\n",
    "model = GenerativeModel(\"gemini-1.5-pro-002\") # Or choose a newer/different version if needed\n",
    "\n",
    "# Initialize GCS Client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
    "print(\"Setup Complete. Vertex AI SDK and GCS Client Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cbb6dfd-81d5-4332-a435-eccb935642d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded: carrier_peak_usage.xlsx\n",
      "✅ Successfully loaded: carrier_account_managers.xlsx\n",
      "✅ Successfully loaded: carrier_first_line_support.xlsx\n",
      "\n",
      "Peak Usage Data Sample:\n",
      "         Date      Time carrier_name  Configured Trunks  Used Trunks  \\\n",
      "0  2023-10-25  14:05:00       5X4 US                1.0          1.0   \n",
      "1  2023-12-11  11:20:00          5x4                3.0          1.0   \n",
      "2  2024-01-24  15:35:00   FATICTICED                4.0          4.0   \n",
      "3  2024-02-15  16:45:00       Tarzon                8.0          2.0   \n",
      "4  2024-02-21  14:35:00          JTT                5.0          5.0   \n",
      "\n",
      "   Configured Capacity  Peak Usage  \n",
      "0                  250      1.0000  \n",
      "1                 1750      0.7789  \n",
      "2                 1800      0.4961  \n",
      "3                  200      1.0000  \n",
      "4                12000      0.3179  \n",
      "\n",
      "Account Managers Data Sample:\n",
      "  carrier_name Your Company Account Manager Name  \\\n",
      "0       5X4 US                       Jhon Walker   \n",
      "1          5x4                       Jhon Walker   \n",
      "2   FATICTICED                       Varun Patel   \n",
      "3       Tarzon                       Jhon Walker   \n",
      "4          JTT                       Varun Patel   \n",
      "\n",
      "  Your Company Account Manager Email Carrier Company Account Manager Name  \\\n",
      "0         jhon.walker@testcomnay.com                             John Doe   \n",
      "1         jhon.walker@testcomnay.com                             John Doe   \n",
      "2         varun.patel@testcomnay.com                           Jane Smith   \n",
      "3         jhon.walker@testcomnay.com                      Michael Johnson   \n",
      "4         varun.patel@testcomnay.com                          Emily Davis   \n",
      "\n",
      "  Carrier Company Account Manager Email  \n",
      "0           john.doe@carriercompany.com  \n",
      "1           john.doe@carriercompany.com  \n",
      "2         jane.smith@carriercompany.com  \n",
      "3    michael.johnson@carriercompany.com  \n",
      "4        emily.davis@carriercompany.com  \n",
      "\n",
      "Support Data Sample:\n",
      "  carrier_name            First Line Contact Name  \\\n",
      "0       5X4 US     Front Line Service Desk 5X4 US   \n",
      "1          5x4        Front Line Service Desk 5x4   \n",
      "2   FATICTICED  Front Line Service Desk FATICTICE   \n",
      "3       Tarzon     Front Line Service Desk Tarzon   \n",
      "4          JTT        Front Line Service Desk JTT   \n",
      "\n",
      "          First Line Contact Email  \n",
      "0      5x4.us@firstlinesupport.com  \n",
      "1         5x4@firstlinesupport.com  \n",
      "2  faticticed@firstlinesupport.com  \n",
      "3      tarzon@firstlinesupport.com  \n",
      "4         jtt@firstlinesupport.com  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_excel_from_gcs(blob_name):\n",
    "    \"\"\"Downloads an Excel file from GCS and loads it into a pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "        blob = bucket.blob(blob_name)\n",
    "        content = blob.download_as_bytes()\n",
    "        df = pd.read_excel(io.BytesIO(content))\n",
    "        print(f\"✅ Successfully loaded: {blob_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {blob_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "df_usage = load_excel_from_gcs(PEAK_USAGE_FILE)\n",
    "df_managers = load_excel_from_gcs(ACCOUNT_MANAGERS_FILE)\n",
    "df_support = load_excel_from_gcs(SUPPORT_FILE)\n",
    "\n",
    "# Display data (if loaded)\n",
    "if df_usage is not None:\n",
    "    print(\"\\nPeak Usage Data Sample:\")\n",
    "    print(df_usage.head())\n",
    "if df_managers is not None:\n",
    "    print(\"\\nAccount Managers Data Sample:\")\n",
    "    print(df_managers.head())\n",
    "if df_support is not None:\n",
    "    print(\"\\nSupport Data Sample:\")\n",
    "    print(df_support.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b00ce5f-0bea-4aec-84d6-f759a4dbc37b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage Data with Standardized Names:\n",
      "Could not display - columns not found\n",
      "\n",
      "Managers Data with Standardized Names:\n",
      "  carrier_name standardized_carrier_name\n",
      "0       5X4 US                    5X4 US\n",
      "1          5x4                       5X4\n",
      "2   FATICTICED                FATICTICED\n",
      "3       Tarzon                    TARZON\n",
      "4          JTT                       JTT\n",
      "\n",
      "Support Data with Standardized Names:\n",
      "Could not display - columns not found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load your data (assuming you've already loaded these)\n",
    "#df_usage = pd.read_excel('carrier_peak_usage.xlsx')\n",
    "#df_managers = pd.read_excel('carrier_account_managers.xlsx')\n",
    "#df_support = pd.read_excel('carrier_first_line_support.xlsx')\n",
    "\n",
    "# Standardize carrier names across all datasets\n",
    "def standardize_carrier_name(name):\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    name = str(name).strip().upper()\n",
    "    # Handle known variations\n",
    "    variations = {\n",
    "        'GT IRELAND': 'GT_IRELAN',\n",
    "        'BLARO_I': 'BLARO_DR',\n",
    "        'BLARO PERU': 'BLARO_PE',\n",
    "        'BLARO ARGENTINA': 'BLARO_AR',\n",
    "        'BLARO CHILE': 'BLARO',\n",
    "        'BLARO BRAZIL': 'BLARO BR',\n",
    "        'BLARO_COLOMBIA': 'BLARO_CO',\n",
    "        'FANX_TELECOM': 'FANX TELE',\n",
    "        'GAROC_TELECOM': 'GAROC TELE',\n",
    "        'KUBAI TELECOM': 'KUBAI TEL',\n",
    "        'TEL_ARABIA': 'TEL ARABIA',\n",
    "        'EL_EGIPT': 'EL EGIPT',\n",
    "        'RANGE_MEDITEL': 'RANGE MEDITEL',\n",
    "        'LOTSWANA_TEL': 'LOTSWANA_TEL'\n",
    "    }\n",
    "    return variations.get(name, name)\n",
    "\n",
    "# Apply standardization to each DataFrame using the correct column names\n",
    "if 'carrier_name_' in df_usage.columns:\n",
    "    df_usage['standardized_carrier_name'] = df_usage['carrier_name_'].apply(standardize_carrier_name)\n",
    "else:\n",
    "    df_usage['standardized_carrier_name'] = None\n",
    "\n",
    "if 'carrier_name' in df_managers.columns:\n",
    "    df_managers['standardized_carrier_name'] = df_managers['carrier_name'].apply(standardize_carrier_name)\n",
    "else:\n",
    "    df_managers['standardized_carrier_name'] = None\n",
    "\n",
    "if 'carrier_name_' in df_support.columns:\n",
    "    df_support['standardized_carrier_name'] = df_support['carrier_name_'].apply(standardize_carrier_name)\n",
    "else:\n",
    "    df_support['standardized_carrier_name'] = None\n",
    "\n",
    "# Display results for verification\n",
    "print(\"\\nUsage Data with Standardized Names:\")\n",
    "if 'carrier_name_' in df_usage.columns and 'standardized_carrier_name' in df_usage.columns:\n",
    "    print(df_usage[['carrier_name_', 'standardized_carrier_name']].head())\n",
    "else:\n",
    "    print(\"Could not display - columns not found\")\n",
    "\n",
    "print(\"\\nManagers Data with Standardized Names:\")\n",
    "if 'carrier_name' in df_managers.columns and 'standardized_carrier_name' in df_managers.columns:\n",
    "    print(df_managers[['carrier_name', 'standardized_carrier_name']].head())\n",
    "else:\n",
    "    print(\"Could not display - columns not found\")\n",
    "\n",
    "print(\"\\nSupport Data with Standardized Names:\")\n",
    "if 'carrier_name_' in df_support.columns and 'standardized_carrier_name' in df_support.columns:\n",
    "    print(df_support[['carrier_name_', 'standardized_carrier_name']].head())\n",
    "else:\n",
    "    print(\"Could not display - columns not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03cc2f50-8ea1-41e9-bb52-45f6b36ab92b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized carrier names in Usage data.\n",
      "Assigned standardized names in Managers data.\n",
      "Standardized carrier names in Support data based on Managers list.\n",
      "\n",
      "Usage Data with Standardized Names:\n",
      "   carrier_name standardized_carrier_name\n",
      "0       5X4 US                    5X4 US\n",
      "1          5x4                       5x4\n",
      "2   FATICTICED                FATICTICED\n",
      "3       Tarzon                    Tarzon\n",
      "4          JTT                       JTT\n"
     ]
    }
   ],
   "source": [
    "def clean_column_names(df):\n",
    "    \"\"\"Standardizes column names (lowercase, replace spaces with underscores).\"\"\"\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "    return df\n",
    "\n",
    "def standardize_carrier_names(df_target, df_reference, target_col, reference_col, threshold=85):\n",
    "    \"\"\"\n",
    "    Standardizes carrier names in df_target based on names in df_reference\n",
    "    using fuzzy matching. Adds a 'standardized_carrier_name' column.\n",
    "    \"\"\"\n",
    "    if df_target is None or df_reference is None:\n",
    "        print(\"One or both dataframes are None, skipping standardization.\")\n",
    "        return df_target\n",
    "\n",
    "    reference_names = df_reference[reference_col].unique().tolist()\n",
    "    standardized_names = {}\n",
    "\n",
    "    for name in df_target[target_col].unique():\n",
    "        if pd.isna(name):\n",
    "            standardized_names[name] = None\n",
    "            continue\n",
    "        # Find the best match above the threshold\n",
    "        match, score = process.extractOne(str(name), reference_names)\n",
    "        if score >= threshold:\n",
    "            standardized_names[name] = match\n",
    "            # print(f\"Matched '{name}' to '{match}' with score {score}\") # Debugging\n",
    "        else:\n",
    "            standardized_names[name] = str(name) # Keep original if no good match\n",
    "            # print(f\"No good match for '{name}' (best: '{match}', score: {score}). Keeping original.\") # Debugging\n",
    "\n",
    "    df_target['standardized_carrier_name'] = df_target[target_col].map(standardized_names)\n",
    "    return df_target\n",
    "\n",
    "# --- Apply Cleaning ---\n",
    "if df_usage is not None:\n",
    "    df_usage = clean_column_names(df_usage)\n",
    "    # Assume 'carrier_name' is the column in df_usage\n",
    "    # Create a reference name list (e.g., from the managers file, assuming it's cleaner or more complete)\n",
    "    if df_managers is not None:\n",
    "      df_managers = clean_column_names(df_managers)\n",
    "      # Assume the column is 'carrier_name' in managers sheet\n",
    "      reference_carrier_names_list = df_managers['carrier_name'].dropna().unique().tolist()\n",
    "\n",
    "      # Standardize usage df based on manager df names\n",
    "      standardized_names_map_usage = {}\n",
    "      usage_carrier_col = 'carrier_name' # Adjust if column name is different after cleaning\n",
    "      if usage_carrier_col in df_usage.columns:\n",
    "          for name in df_usage[usage_carrier_col].unique():\n",
    "                if pd.isna(name): continue\n",
    "                match, score = process.extractOne(str(name), reference_carrier_names_list)\n",
    "                if score >= 85: # Adjust threshold as needed\n",
    "                    standardized_names_map_usage[name] = match\n",
    "                else:\n",
    "                    standardized_names_map_usage[name] = str(name) # Keep original if no good match\n",
    "          df_usage['standardized_carrier_name'] = df_usage[usage_carrier_col].map(standardized_names_map_usage)\n",
    "          print(\"Standardized carrier names in Usage data.\")\n",
    "      else:\n",
    "          print(f\"Column '{usage_carrier_col}' not found in usage dataframe after cleaning.\")\n",
    "          # Handle error or assign a default standardized name column\n",
    "          df_usage['standardized_carrier_name'] = df_usage[usage_carrier_col] if usage_carrier_col in df_usage.columns else None\n",
    "\n",
    "    else: # If manager df is not available, standardize based on its own names (less ideal)\n",
    "       print(\"Manager data not loaded. Standardizing Usage data based on its own unique names.\")\n",
    "       df_usage['standardized_carrier_name'] = df_usage['carrier_name'] # Or apply self-referential fuzzy matching if needed\n",
    "\n",
    "\n",
    "# Standardize managers and support dfs (use one as the primary reference or self-reference)\n",
    "if df_managers is not None:\n",
    "   # Standardize against itself or a master list if you have one\n",
    "   df_managers['standardized_carrier_name'] = df_managers['carrier_name'] # Simplest approach: assume names are already the reference standard\n",
    "   print(\"Assigned standardized names in Managers data.\")\n",
    "\n",
    "if df_support is not None:\n",
    "    df_support = clean_column_names(df_support)\n",
    "    support_carrier_col = 'carrier_name' # Adjust if needed\n",
    "    if support_carrier_col in df_support.columns:\n",
    "      if df_managers is not None: # Use managers list as reference if available\n",
    "          standardized_names_map_support = {}\n",
    "          for name in df_support[support_carrier_col].unique():\n",
    "                if pd.isna(name): continue\n",
    "                match, score = process.extractOne(str(name), reference_carrier_names_list)\n",
    "                if score >= 85:\n",
    "                    standardized_names_map_support[name] = match\n",
    "                else:\n",
    "                    standardized_names_map_support[name] = str(name)\n",
    "          df_support['standardized_carrier_name'] = df_support[support_carrier_col].map(standardized_names_map_support)\n",
    "          print(\"Standardized carrier names in Support data based on Managers list.\")\n",
    "\n",
    "      else: # Fallback to self-reference\n",
    "          df_support['standardized_carrier_name'] = df_support[support_carrier_col]\n",
    "          print(\"Standardized carrier names in Support data based on its own names.\")\n",
    "    else:\n",
    "       print(f\"Column '{support_carrier_col}' not found in support dataframe after cleaning.\")\n",
    "       df_support['standardized_carrier_name'] = None\n",
    "\n",
    "\n",
    "# Display standardized names (optional check)\n",
    "if df_usage is not None: print(\"\\nUsage Data with Standardized Names:\\n\", df_usage[['carrier_name', 'standardized_carrier_name']].head())\n",
    "# ... similar checks for df_managers and df_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46e49d0-bacd-4231-945f-630a4da66798",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage columns: ['date', 'time', 'carrier_name', 'configured_trunks', 'used_trunks', 'configured_capacity', 'peak_usage', 'standardized_carrier_name']\n",
      "Managers columns: ['carrier_name', 'your_company_account_manager_name', 'your_company_account_manager_email', 'carrier_company_account_manager_name', 'carrier_company_account_manager_email', 'standardized_carrier_name']\n",
      "Support columns: ['carrier_name', 'first_line_contact_name', 'first_line_contact_email', 'standardized_carrier_name']\n"
     ]
    }
   ],
   "source": [
    "print(\"Usage columns:\", df_usage.columns.tolist())\n",
    "print(\"Managers columns:\", df_managers.columns.tolist())\n",
    "print(\"Support columns:\", df_support.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "194ff54c-bae9-4c0c-bbaf-1bd7ea1f37fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Manager data.\n",
      "Merged Support data.\n",
      "\n",
      "Merged Data Head:\n",
      "          date      time carrier_name  configured_trunks  used_trunks  \\\n",
      "0  2023-10-25  14:05:00       5X4 US                1.0          1.0   \n",
      "1  2023-12-11  11:20:00          5x4                3.0          1.0   \n",
      "2  2024-01-24  15:35:00   FATICTICED                4.0          4.0   \n",
      "3  2024-02-15  16:45:00       Tarzon                8.0          2.0   \n",
      "4  2024-02-21  14:35:00          JTT                5.0          5.0   \n",
      "\n",
      "   configured_capacity  peak_usage standardized_carrier_name  \\\n",
      "0                  250      1.0000                    5X4 US   \n",
      "1                 1750      0.7789                       5x4   \n",
      "2                 1800      0.4961                FATICTICED   \n",
      "3                  200      1.0000                    Tarzon   \n",
      "4                12000      0.3179                       JTT   \n",
      "\n",
      "  your_company_account_manager_name your_company_account_manager_email  \\\n",
      "0                       Jhon Walker         jhon.walker@testcomnay.com   \n",
      "1                       Jhon Walker         jhon.walker@testcomnay.com   \n",
      "2                       Varun Patel         varun.patel@testcomnay.com   \n",
      "3                       Jhon Walker         jhon.walker@testcomnay.com   \n",
      "4                       Varun Patel         varun.patel@testcomnay.com   \n",
      "\n",
      "  carrier_company_account_manager_name carrier_company_account_manager_email  \\\n",
      "0                             John Doe           john.doe@carriercompany.com   \n",
      "1                             John Doe           john.doe@carriercompany.com   \n",
      "2                           Jane Smith         jane.smith@carriercompany.com   \n",
      "3                      Michael Johnson    michael.johnson@carriercompany.com   \n",
      "4                          Emily Davis        emily.davis@carriercompany.com   \n",
      "\n",
      "             first_line_contact_name         first_line_contact_email  \n",
      "0     Front Line Service Desk 5X4 US      5x4.us@firstlinesupport.com  \n",
      "1        Front Line Service Desk 5x4         5x4@firstlinesupport.com  \n",
      "2  Front Line Service Desk FATICTICE  faticticed@firstlinesupport.com  \n",
      "3     Front Line Service Desk Tarzon      tarzon@firstlinesupport.com  \n",
      "4        Front Line Service Desk JTT         jtt@firstlinesupport.com  \n"
     ]
    }
   ],
   "source": [
    "# Merge the dataframes using the standardized carrier name\n",
    "df_merged = None\n",
    "if df_usage is not None and 'standardized_carrier_name' in df_usage.columns:\n",
    "    df_merged = df_usage\n",
    "\n",
    "    if df_managers is not None and 'standardized_carrier_name' in df_managers.columns:\n",
    "        # Select only necessary manager columns and rename before merge to avoid conflicts if needed\n",
    "        manager_cols = ['standardized_carrier_name', 'your_company_account_manager_name', 'your_company_account_manager_email', 'carrier_company_account_manager_name', 'carrier_company_account_manager_email']\n",
    "        df_managers_subset = df_managers[manager_cols].drop_duplicates(subset=['standardized_carrier_name'])\n",
    "        df_merged = pd.merge(df_merged, df_managers_subset, on='standardized_carrier_name', how='left')\n",
    "        print(\"Merged Manager data.\")\n",
    "\n",
    "    if df_support is not None and 'standardized_carrier_name' in df_support.columns:\n",
    "         # Select support columns and rename before merge\n",
    "        support_cols = ['standardized_carrier_name', 'first_line_contact_name', 'first_line_contact_email']\n",
    "        df_support_subset = df_support[support_cols].drop_duplicates(subset=['standardized_carrier_name'])\n",
    "        df_merged = pd.merge(df_merged, df_support_subset, on='standardized_carrier_name', how='left')\n",
    "        print(\"Merged Support data.\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot merge data as Usage data or standardized name column is missing.\")\n",
    "\n",
    "if df_merged is not None:\n",
    "    print(\"\\nMerged Data Head:\\n\", df_merged.head())\n",
    "    # Handle potential NaN values introduced by merging (optional: fill with defaults like 'N/A')\n",
    "    # df_merged = df_merged.fillna('N/A')\n",
    "else:\n",
    "    print(\"Merging failed or was skipped.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87fb0d26-9a21-4c82-bf70-958dd7c1648c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identified 76 underutilized carriers (Usage < 40.0%).\n",
      "Underutilized Carriers Sample:\n",
      "   standardized_carrier_name  configured_capacity  peak_usage  \\\n",
      "0                    5X4 US                  250      1.0000   \n",
      "1                       5x4                 1750      0.7789   \n",
      "2                FATICTICED                 1800      0.4961   \n",
      "3                    Tarzon                  200      1.0000   \n",
      "4                       JTT                12000      0.3179   \n",
      "\n",
      "   usage_percentage  proposed_capacity  \n",
      "0          0.400000                125  \n",
      "1          0.044509                875  \n",
      "2          0.027561                900  \n",
      "3          0.500000                100  \n",
      "4          0.002649               6000  \n"
     ]
    }
   ],
   "source": [
    "df_analysis = None\n",
    "if df_merged is not None and 'peak_usage' in df_merged.columns and 'configured_capacity' in df_merged.columns:\n",
    "    # Make a copy for analysis to avoid modifying the merged view directly\n",
    "    df_analysis = df_merged.copy()\n",
    "\n",
    "    # Ensure numeric types and handle potential errors/zeros\n",
    "    df_analysis['peak_usage'] = pd.to_numeric(df_analysis['peak_usage'], errors='coerce')\n",
    "    df_analysis['configured_capacity'] = pd.to_numeric(df_analysis['configured_capacity'], errors='coerce')\n",
    "    df_analysis.dropna(subset=['peak_usage', 'configured_capacity'], inplace=True) # Drop rows where conversion failed\n",
    "    df_analysis = df_analysis[df_analysis['configured_capacity'] > 0] # Avoid division by zero\n",
    "\n",
    "    # Calculate Usage Percentage\n",
    "    df_analysis['usage_percentage'] = (df_analysis['peak_usage'] / df_analysis['configured_capacity']) * 100\n",
    "\n",
    "    # Identify Underutilized Carriers\n",
    "    df_underutilized = df_analysis[df_analysis['usage_percentage'] < USAGE_THRESHOLD_PERCENT].copy()\n",
    "\n",
    "    # Calculate Proposed New Capacity\n",
    "    df_underutilized['proposed_capacity'] = (df_underutilized['configured_capacity'] * CAPACITY_REDUCTION_FACTOR).round().astype(int) # Round to nearest int\n",
    "\n",
    "    print(f\"\\nIdentified {len(df_underutilized)} underutilized carriers (Usage < {USAGE_THRESHOLD_PERCENT}%).\")\n",
    "    if not df_underutilized.empty:\n",
    "        print(\"Underutilized Carriers Sample:\\n\", df_underutilized[['standardized_carrier_name', 'configured_capacity', 'peak_usage', 'usage_percentage', 'proposed_capacity']].head())\n",
    "else:\n",
    "    print(\"Cannot perform analysis. Merged data or required columns ('peak_usage', 'configured_capacity') are missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5fb93a-8698-4c87-9bce-42e1a5c3088b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_chatbot(query):\n",
    "    \"\"\"Sends a query with context to the LLM and returns the answer.\"\"\"\n",
    "    if df_merged is None:\n",
    "        return \"Sorry, the data is not loaded or merged correctly. I cannot answer questions yet.\"\n",
    "\n",
    "    # --- Basic Context Preparation ---\n",
    "    # For more complex queries, you might need to dynamically select relevant data\n",
    "    # For now, provide a general overview and let the LLM figure it out.\n",
    "    # You could enhance this to find specific carrier data if the query mentions one.\n",
    "\n",
    "    context_prompt = f\"\"\"\n",
    "    You are a helpful AI assistant for a telecom solutions architect. You have access to data about Public IP Carriers.\n",
    "    The data includes carrier names, peak SIP session usage, configured capacity, account manager details (both internal and carrier-side), and first-line support contacts.\n",
    "    The data comes from three sources and has been merged. Carrier names have been standardized.\n",
    "\n",
    "    Use the provided data snapshot (if any) and your general knowledge to answer the user's query accurately and professionally.\n",
    "\n",
    "    Available Data Columns Overview: {', '.join(df_merged.columns.tolist()) if df_merged is not None else 'Data not available'}\n",
    "    Total Carriers in Merged Data: {len(df_merged['standardized_carrier_name'].unique()) if df_merged is not None else 0}\n",
    "\n",
    "    User Query: {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Optional: Add Specific Data Snippet if Query is Specific ---\n",
    "    # Example: if query mentions a carrier name, find its row(s) and add to context\n",
    "    # carrier_match = re.search(r'carrier\\s+([A-Za-z0-9_\\-\\s]+)', query, re.IGNORECASE)\n",
    "    # if carrier_match:\n",
    "    #    carrier_name_query = carrier_match.group(1).strip()\n",
    "    #    # Try finding the carrier using standardized name\n",
    "    #    carrier_data = df_merged[df_merged['standardized_carrier_name'].str.contains(carrier_name_query, case=False, na=False)]\n",
    "    #    if not carrier_data.empty:\n",
    "    #       context_prompt += f\"\\n\\nRelevant Data for '{carrier_name_query}':\\n{carrier_data.to_string()}\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(context_prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Vertex AI Model: {e}\")\n",
    "        return \"Sorry, I encountered an error trying to process your request with the AI model.\"\n",
    "\n",
    "# --- Example Usage (in a notebook cell) ---\n",
    "# user_question = \"Tell me about the peak usage for BLARO ARGENTINA.\"\n",
    "# answer = ask_chatbot(user_question)\n",
    "# print(answer)\n",
    "\n",
    "# user_question = \"Who is the account manager from our company for Carrier XYZ?\"\n",
    "# answer = ask_chatbot(user_question)\n",
    "# print(answer)\n",
    "\n",
    "# user_question = \"How many carriers are there in total?\"\n",
    "# answer = ask_chatbot(user_question)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebc89d1-cd68-46c2-b6a5-a14f8e61e7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_capacity_reduction_email(carrier_info):\n",
    "    \"\"\"Generates an email notification using the LLM based on carrier data.\"\"\"\n",
    "\n",
    "    # Ensure all needed info is present, handle potential missing data gracefully\n",
    "    carrier_name = carrier_info.get('standardized_carrier_name', 'N/A')\n",
    "    current_capacity = carrier_info.get('configured_capacity', 'N/A')\n",
    "    peak_usage = carrier_info.get('peak_usage', 'N/A')\n",
    "    usage_percent = carrier_info.get('usage_percentage', 'N/A')\n",
    "    new_capacity = carrier_info.get('proposed_capacity', 'N/A')\n",
    "\n",
    "    your_am_name = carrier_info.get('your_company_account_manager_name', 'Your Account Manager')\n",
    "    your_am_email = carrier_info.get('your_company_account_manager_email', 'your_am@yourcompany.com') # Provide a default or fetch dynamically\n",
    "    carrier_am_name = carrier_info.get('carrier_company_account_manager_name', 'Carrier Contact')\n",
    "    carrier_am_email = carrier_info.get('carrier_company_account_manager_email', '') # Email is crucial\n",
    "    support_name = carrier_info.get('first_line_contact_name', 'Carrier Support')\n",
    "    support_email = carrier_info.get('first_line_contact_email', '') # Email is crucial\n",
    "\n",
    "\n",
    "    # Construct the prompt for the LLM\n",
    "    email_prompt = f\"\"\"\n",
    "    Generate a professional email notification regarding a planned capacity reduction for a Public IP Carrier voice trunk.\n",
    "\n",
    "    **Instructions:**\n",
    "    1.  Be polite and professional.\n",
    "    2.  Clearly state the reason for the reduction (peak usage consistently below {USAGE_THRESHOLD_PERCENT}% of configured capacity).\n",
    "    3.  Mention the current configured capacity, the observed peak usage (and percentage), and the proposed new capacity (which is 50% of the current).\n",
    "    4.  Address the email primarily to the Carrier Company Account Manager.\n",
    "    5.  CC the Account Manager from our company and the Carrier's First Line Support contact.\n",
    "    6.  Provide contact information (Our Company's AM) for questions or discussion.\n",
    "    7.  Suggest a timeframe for discussion before the change is implemented (e.g., \"within the next two weeks\").\n",
    "\n",
    "    **Carrier Details:**\n",
    "    * Carrier Name: {carrier_name}\n",
    "    * Current Configured Capacity: {current_capacity} sessions\n",
    "    * Observed Peak Usage: {peak_usage:.0f} sessions ({usage_percent:.1f}%)\n",
    "    * Proposed New Capacity: {new_capacity} sessions\n",
    "    * Carrier Account Manager: {carrier_am_name} ({carrier_am_email})\n",
    "    * Our Account Manager: {your_am_name} ({your_am_email})\n",
    "    * Carrier First Line Support: {support_name} ({support_email})\n",
    "\n",
    "    **Generate the email with a clear Subject line and Body.**\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if essential email addresses are present\n",
    "    if not carrier_am_email:\n",
    "        return {\"error\": f\"Missing Carrier Account Manager email for {carrier_name}. Cannot generate email.\"}\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(email_prompt)\n",
    "\n",
    "        # Basic parsing attempt (assuming Subject: ... Body: ...)\n",
    "        email_text = response.text\n",
    "        subject = f\"Planned Capacity Adjustment for {carrier_name} Voice Trunk\" # Default subject\n",
    "        body = email_text\n",
    "\n",
    "        # Try to extract Subject if model provides it explicitly\n",
    "        subject_match = re.search(r\"Subject:\\s*(.*)\", email_text, re.IGNORECASE)\n",
    "        if subject_match:\n",
    "            subject = subject_match.group(1).strip()\n",
    "            # Remove subject line from body if found\n",
    "            body = re.sub(r\"Subject:\\s*.*\\n?\", \"\", body, flags=re.IGNORECASE).strip()\n",
    "            body = re.sub(r\"Body:\\s*\\n?\", \"\", body, flags=re.IGNORECASE).strip() # Remove Body: tag if present\n",
    "\n",
    "\n",
    "        cc_emails = [e for e in [your_am_email, support_email] if pd.notna(e) and '@' in str(e)] # Filter valid emails\n",
    "\n",
    "        return {\n",
    "            \"to\": carrier_am_email,\n",
    "            \"cc\": cc_emails,\n",
    "            \"subject\": subject,\n",
    "            \"body\": body,\n",
    "            \"carrier\": carrier_name\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Vertex AI Model for email generation: {e}\")\n",
    "        return {\"error\": f\"LLM error generating email for {carrier_name}.\"}\n",
    "\n",
    "\n",
    "# --- Example Usage (in a notebook cell) ---\n",
    "# if df_underutilized is not None and not df_underutilized.empty:\n",
    "#     # Generate for the first underutilized carrier\n",
    "#     first_carrier_info = df_underutilized.iloc[0].to_dict()\n",
    "#     generated_email = generate_capacity_reduction_email(first_carrier_info)\n",
    "#     if \"error\" in generated_email:\n",
    "#         print(generated_email[\"error\"])\n",
    "#     else:\n",
    "#         print(f\"--- Generated Email for {generated_email['carrier']} ---\")\n",
    "#         print(f\"To: {generated_email['to']}\")\n",
    "#         print(f\"CC: {', '.join(generated_email['cc'])}\")\n",
    "#         print(f\"Subject: {generated_email['subject']}\")\n",
    "#         print(\"\\nBody:\\n\", generated_email['body'])\n",
    "# else:\n",
    "#      print(\"No underutilized carriers found to generate emails for.\")\n",
    "\n",
    "# --- Generate for ALL underutilized carriers ---\n",
    "# all_generated_emails = []\n",
    "# if df_underutilized is not None and not df_underutilized.empty:\n",
    "#    print(\"\\n--- Generating Emails for All Underutilized Carriers ---\")\n",
    "#    for index, row in df_underutilized.iterrows():\n",
    "#        carrier_info = row.to_dict()\n",
    "#        email_data = generate_capacity_reduction_email(carrier_info)\n",
    "#        all_generated_emails.append(email_data)\n",
    "#        if \"error\" in email_data:\n",
    "#             print(f\"Failed for {carrier_info.get('standardized_carrier_name', 'Unknown')}: {email_data['error']}\")\n",
    "#        else:\n",
    "#             print(f\"Successfully generated email draft for {email_data['carrier']}\")\n",
    "#    # Now you have a list 'all_generated_emails' containing dicts for each email (or errors)\n",
    "#    # You can review them before sending.\n",
    "# else:\n",
    "#    print(\"No underutilized carriers found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048c18c0-4421-420a-8339-4091d4ca1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example using smtplib (Requires SMTP server access & credentials) ---\n",
    "# import smtplib\n",
    "# from email.mime.text import MIMEText\n",
    "# from email.mime.multipart import MIMEMultipart\n",
    "# from google.cloud import secretmanager # Recommended for credentials\n",
    "\n",
    "# def get_secret(secret_id, version_id=\"latest\"):\n",
    "#     \"\"\"Retrieves a secret from Google Secret Manager.\"\"\"\n",
    "#     client = secretmanager.SecretManagerServiceClient()\n",
    "#     name = f\"projects/{PROJECT_ID}/secrets/{secret_id}/versions/{version_id}\"\n",
    "#     response = client.access_secret_version(request={\"name\": name})\n",
    "#     return response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "# def send_email_smtp(to_email, cc_emails, subject, body):\n",
    "#     # --- Retrieve Credentials Securely ---\n",
    "#     # SMTP_SERVER = \"smtp.yourprovider.com\"\n",
    "#     # SMTP_PORT = 587 # Or 465 for SSL\n",
    "#     # SENDER_EMAIL = get_secret(\"your-sender-email-secret-id\") # Store in Secret Manager\n",
    "#     # SENDER_PASSWORD = get_secret(\"your-sender-password-secret-id\") # Store in Secret Manager\n",
    "\n",
    "#     try:\n",
    "#         msg = MIMEMultipart()\n",
    "#         msg['From'] = SENDER_EMAIL\n",
    "#         msg['To'] = to_email\n",
    "#         msg['Cc'] = \", \".join(cc_emails)\n",
    "#         msg['Subject'] = subject\n",
    "#         msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "#         server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n",
    "#         server.starttls() # Use TLS\n",
    "#         server.login(SENDER_EMAIL, SENDER_PASSWORD)\n",
    "#         recipients = [to_email] + cc_emails\n",
    "#         text = msg.as_string()\n",
    "#         server.sendmail(SENDER_EMAIL, recipients, text)\n",
    "#         server.quit()\n",
    "#         print(f\"Email sent successfully to {to_email}\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error sending email to {to_email}: {e}\")\n",
    "#         return False\n",
    "\n",
    "# # --- Usage after generating emails ---\n",
    "# for email_data in all_generated_emails:\n",
    "#    if \"error\" not in email_data:\n",
    "#        print(f\"\\nAttempting to send email for {email_data['carrier']}...\")\n",
    "#        # UNCOMMENT BELOW TO ACTUALLY SEND - USE WITH CAUTION\n",
    "#        # send_email_smtp(email_data['to'], email_data['cc'], email_data['subject'], email_data['body'])\n",
    "#        # print(\"--- Email sending commented out for safety ---\")\n",
    "#        pass # Keep sending commented out initially\n",
    "#    else:\n",
    "#        print(f\"Skipping send for {email_data.get('carrier', 'Unknown')} due to generation error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e68555-80e7-4528-a89f-cc7e383d648f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-17.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-17:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
